strategy: ddp
benchmark: True
pretrained: False
sync_batchnorm: False
# --------------------------------------
# dataset parameters
# --------------------------------------
dataset_name: imagenet
image_size: 224
#multi_scale: null
multi_scale: !!str 192_280   # image_size 192 before epoch 280
test_crop_ratio: 0.9
# --------------------------------------
# Traning parameters
# --------------------------------------
num_workers: 8       # number of threads
batch_size: 256      # 'batch_size for traning'
batch_size_eva: 256  #'batch_size for evaluation'
# --------------------------------------
# Optimizer parameters
# --------------------------------------
epochs: 300
opt: adamw
weight_decay: 0.01
momentum: 0.9
clip_grad: null
precision: 16
# --------------------------------------
# Learning rate schedule parameters
# --------------------------------------
sched: cosine
lr: 0.004   # for bs=4096
warmup: True
warmup_lr: 0.000001
min_lr: 0.00001
warmup_epochs: 20
# --------------------------------------
# Distillation parameters
# --------------------------------------
teacher_model: regnety_160
distillation_type: none   # do not use KD by default
#distillation_type: hard  # should be better than soft
#distillation_type: soft
distillation_alpha: 0.5
distillation_tau: 1.0
# --------------------------------------
# Model parameters
# --------------------------------------
model_name: partialnet
mlp_ratio: 2
embed_dim: 48
depths: [1, 2, 8, 2]
feature_dim: 1280
patch_size: 4
patch_stride: 4
patch_size2: 2
patch_stride2: 2
layer_scale_init_value: 0 # no layer scale
drop_path_rate: 0.02
norm_layer:  BN
act_layer: GELU
n_div: 4

# --------------------------------------
# Dynamic partial convolution parameters
# --------------------------------------
auto_div: False           # Enable dynamic partial convolution
u_regular: True           # Enable regularization for layer-by-layer group convolution
u_regular_b: 2            # Control the complexity of the expected model 2,4,8,10
u_regular_theta: 0        # The default value is 0.5 and 0 means closed.
penalize_alpha: -0.01     # l_gate/u_regular的 penalize因子 -1, -0.07, -0.02
l_gate: True              # Enable reasonable of order loss calculation 
index_div: False  
loss_gate_alpha: 1 
print_n_div: False 
print_loss_gate: False
pre_epoch: 0              # Fixed DPConv training epoch number

# --------------------------------------
# Partial visual ATtention mechanism
# --------------------------------------
use_channel_attn: True
use_spatial_attn: True
patnet_t0: True

# --------------------------------------
# Augmentation parameters
# --------------------------------------
color_jitter: 0
aa: rand-m3-mstd0.5-inc1  # Use AutoAugment policy, Rand Augment
train_interpolation: bicubic    # Training interpolation (random, bilinear, bicubic default: "bicubic")
smoothing: 0.1    # Label smoothing
# Random Erase params
reprob: 0
remode: pixel
recount: 1
# --------------------------------------
# MixUp/CutMix parameters
# --------------------------------------
mixup: 0.1
cutmix: 1.0
cutmix_minmax: null   # cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)
mixup_prob: 1.0         # Probability of performing mixup or cutmix when either/both is enabled
mixup_switch_prob: 0.5  # Probability of switching to cutmix when both mixup and cutmix enabled
mixup_mode: batch       # How to apply mixup/cutmix params. Per "batch", "pair", or "elem"