strategy: ddp #ddp, ddp_spawn, deepspeed, dp
benchmark: True  # Benchmark mode can be enabled for GPU inference 就是打开torch.backends.cudnn.benchmark.
pretrained: False
sync_batchnorm: False
# --------------------------------------
# dataset parameters
# --------------------------------------
dataset_name: cifar10 # dataset: cifar10, imagenet
image_size: 32  #32, 224
multi_scale: null
# multi_scale: !!str 192_280   # image_size 192 before epoch 280
test_crop_ratio: 0.9
# --------------------------------------
# Traning parameters
# --------------------------------------
num_workers: 8       # number of threads
batch_size: 256      # 'batch_size for traning'
batch_size_eva: 256  #'batch_size for evaluation'
# --------------------------------------
# Optimizer parameters
# --------------------------------------
epochs: 300
opt: adamw
weight_decay: 0.005
momentum: 0.9   
clip_grad: null
precision: 16 # full precision (32), half precision (16) or bfloat16 precision (bf16)
# --------------------------------------
# Learning rate schedule parameters
# --------------------------------------
sched: cosine
lr: 0.0005   # for bs=4096
warmup: True
warmup_lr: 0.000001
min_lr: 0.00001
warmup_epochs: 20
# --------------------------------------
# Distillation parameters
# --------------------------------------
teacher_model: regnety_160
distillation_type: none   # do not use KD by default
#distillation_type: hard  # should be better than soft
#distillation_type: soft
distillation_alpha: 0.5
distillation_tau: 1.0
# --------------------------------------
# Model parameters
# --------------------------------------
model_name: partialnet  # Model:  'partialnet' 'convnext_tiny' etc.
mlp_ratio: 2
embed_dim: 32              # 32  2^(n) for DPConv  40 for PartialNet
depths: [1, 2, 8, 2]
feature_dim: 1280
patch_size: 4
patch_stride: 4
patch_size2: 2
patch_stride2: 2
layer_scale_init_value: 0 # no layer scale
drop_path_rate: 0.
norm_layer:  BN
act_layer: GELU
n_div: 4                  # partial ratio:  2,4,8

# --------------------------------------
# Dynamic partial convolution parameters
# --------------------------------------
auto_div: False           # Enable dynamic partial convolution
u_regular: True           # Enable regularization for layer-by-layer group convolution
u_regular_b: 2            # Control the complexity of the expected model 2,4,8,10
u_regular_theta: 0        # The default value is 0.5 and 0 means closed.
penalize_alpha: -0.01     # l_gate/u_regular的 penalize因子 -1, -0.07, -0.02
l_gate: True              # Enable reasonable of order loss calculation 
index_div: False  
loss_gate_alpha: 1 
print_n_div: False 
print_loss_gate: False
pre_epoch: 0              # Fixed DPConv training epoch number

# --------------------------------------
# Partial visual ATtention mechanism
# --------------------------------------
use_channel_attn: True
use_spatial_attn: False
patnet_t0: True
# --------------------------------------
# Augmentation parameters
# --------------------------------------
color_jitter: 0
aa: null
train_interpolation: bicubic    # Training interpolation (random, bilinear, bicubic default: "bicubic")
smoothing: 0.1                  # Label smoothing
# Random Erase params
reprob: 0
remode: pixel
recount: 1
# --------------------------------------
# MixUp/CutMix parameters
# --------------------------------------
mixup: 0.05             # If > 0, mixup is active
cutmix: 1.0             # If > 0, cutmix is ​​active
cutmix_minmax: null     # cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)
mixup_prob: 1.0         # Probability of performing mixup or cutmix when either/both is enabled
mixup_switch_prob: 0.5  # Probability of switching to cutmix when both mixup and cutmix enabled
mixup_mode: batch       # How to apply mixup/cutmix params. Per "batch", "pair", or "elem"